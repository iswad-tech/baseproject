sudo apt update
sudo apt upgrade
sudo apt autoremove

Create the volume for shared folder:
sudo gluster volume create app-volume IP_ADDRESS:/gluster/brick1 force

Start the volume:
sudo gluster volume start app-volume

Check the status of volume:
sudo gluster volume info

Mount the volume:
sudo mount -t glusterfs IP_ADDRESS:/app-volume /var/www/app

In order to automatically mount the volume after reboot go to /etc/fstab and add:
sudo nano /etc/fstab
IP_ADDRESS:/app-volume /var/www/app glusterfs defaults,_netdev 0 0
Then run:
sudo nano /etc/systemd/system/mount-glusterfs.service
Add the following content to it: (between two lines)
-----------------------------------
[Unit]
Description=Mount GlusterFS Volume
After=network-online.target
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/bin/mount -a
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target

------------------------------------
Then run:
sudo systemctl enable mount-glusterfs.service


sudo chown USER_NAME:USER_NAME /var/www/app
git clone SSH_REPO_URL .

Create the following subfolders:
./api/vol/static/
./api/vol/media/

Go to /var/www/app and run
./automation.sh --> 10

Update following files:
.env
config/envFiles/django/prod/.env
config/envFiles/postgres/prod/.env
client/next.config.js
redis/redis.conf => requirepass should be equal to redis pass in django env file
init-letsencrypt.sh => Other than domain change the USER_NAME as well
use_backup_db.sh => change DB_USER and DB_NAME
utils/shellScripting/constants/env.sh
redeploy-swarm.sh => Change USER_NAME
backup_db_swarm.sh => Change DB_NAME, DB_USER, and USER_NAME
nginx/default-swarm.conf => Change APP_URL
backup_db.sh => Change DB_NAME and DB_USER

Go to /var/www/app folder and run
sudo chmod +x ./init-letsencrypt.sh
sudo chmod +x ./backup_db.sh
sudo chmod +x ./use_backup_db.sh
sudo chmod +x ./redeploy-swarm.sh
sudo chmod +x ./backup_db_swarm.sh

sudo apt-get install apache2-utils

Then go to /var/www/app/nginx folder and run
htpasswd -c htpasswd CELERY_FLOWER_USER
Then use `CELERY_FLOWER_PASSWORD` you defined in the env variables.

Create the following subfolders: <br>
./nginx/certbot/conf/
./nginx/certbot/www/
./volume_data/django_static
./volume_data/pg_data

Then go to /var/www/app and run
docker-compose -f docker-compose-createSSL.yml up --build -d

Add A records to the DNS settings of relevant domain pointing to the server IP address.
Note: www also must be referring to the server ip address

Then run:
sudo ./init-letsencrypt.sh

Now the app should work perfectly

In order to use docker swarm which us highly recommended:
remove existing containers and images by following command:
docker container rm -f $(docker container ls -a -q)
docker image rm -f $(docker image ls -a -q)
docker-compose -f docker-compose-prod-ssl.yml down
Create 3 repos for clent, api, and nginx on docker hub
Then run:
docker login -u DOCKER_HUB_USER_NAME

Run:
docker swarm init --advertise-addr IP_ADDRESS

in your local update
utils/shellScripting/constants/constants.sh
utils/shellScripting/constants/env.sh
Then using automation.sh you can push it to the server

Add the followings to cron job:
0 4 * * * /var/www/app/backup_db_swarm.sh
0 6 * * * /var/www/app/redeploy-swarm.sh
0 7 * * * docker system prune -a --volumes -f

Create the following folders on the home directory of the USER_NAME:
cron_commands.log (A file) => sudo touch cron_commands.log => sudo chown USER_NAME:USER_NAME cron_commands.log
db_backups (A folder) => sudo mkdir db_backups => sudo chown USER_NAME:USER_NAME db_backups
media_backups (A folder) => sudo mkdir media_backups => sudo chown USER_NAME:USER_NAME media_backups

After Reboot if the volume is not automatically mounted, run the following command:
sudo mount -t glusterfs IP_ADDRESS:/app-volume /var/www/app

-----------------------------------------------------------
Adding new nodes:
Run the following commands in the new node:
sudo apt update
sudo apt upgrade
sudo apt autoremove

On existing GlusterFS node, run the following command to peer the new node:
sudo gluster peer probe IP_ADDRESS

Check the status:
sudo gluster peer status

Then Run the following command on the GlusterFS node::
sudo gluster volume add-brick app-volume replica NEW_REPLICA_COUNT NEW_NODE_IP_ADDRESS_1:/gluster/brick1 NEW_NODE_IP_ADDRESS_2:/gluster/brick1 force

For examle: sudo gluster volume add-brick app-volume replica 3 NEW_NODE_IP_ADDRESS_1:/gluster/brick1 NEW_NODE_IP_ADDRESS_2:/gluster/brick1 force

After adding the new brick, you should mount the volume across the new nodes:
sudo mount -t glusterfs MANAGERIP_ADDRESS:/app-volume /var/www/app

In order to automatically mount the volume after reboot go to /etc/fstab and add:
sudo nano /etc/fstab
IP_ADDRESS:/app-volume /var/www/app glusterfs defaults,_netdev 0 0
Then run:
sudo nano /etc/systemd/system/mount-glusterfs.service
Add the following content to it: (between two lines)
-----------------------------------
[Unit]
Description=Mount GlusterFS Volume
After=network-online.target
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/bin/mount -a
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target

------------------------------------
Then run:
sudo systemctl enable mount-glusterfs.service

Then in order to add manager node to docker swarm run the following command in the main manager node:
docker swarm join-token manager

Copy the output and paste it to the new manager nodes

In order to add worker nodes, run the following command in the main manager node:
docker swarm join-token worker

Copy the output and paste it to the new worker nodes

Run docker node ls to see the status of nodes

Then create a load balancer in digital ocean and connect all the swarm nodes to it

Then you need to change the A record in DNS settings to point out to the load balancer

Then restart the application again using swarm

In order to check how app is distributed across all nodes, run the following command:
docker stack services app --format "{{.Name}}" | xargs -I {} docker service ps {} --filter "desired-state=running"

And in order to see what apps are running in the current node, run the following:
docker node ps self --filter "desired-state=running"

You can do Force update for all services: 
for service in $(docker service ls --format "{{.Name}}"); do docker service update --force $service; done
